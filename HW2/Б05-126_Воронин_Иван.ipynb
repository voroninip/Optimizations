{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3a6fa0",
   "metadata": {},
   "source": [
    "# Домашнее задание 2\n",
    "\n",
    "Это домашнее задание по материалам 3-5 недели семестра (3-5 лекции и 3-4 семинары). Дедлайн по отправке - 23:59 18 октября.\n",
    "\n",
    "- Домашнее задание выполняется в этом же Jupyter Notebook'e.\n",
    "\n",
    "- Файл необходимо переименовать: __Номер группы_Фамилия_Имя__ (без пробелов в начале и конце). Пример: __Б05-100_Иванов_Иван__.\n",
    "\n",
    "- ДЗ нужно отправлять на __OptimizationHomework@yandex.ru__. Тема письма: __МФТИ_номер задания__ (без пробелов в начале и конце). Для данного ДЗ тема письма: __МФТИ_2__.\n",
    "\n",
    "- Для решения можно использовать Google Colab, но присылать нужно не ссылку на Colab, а готовый notebook и все необходимые дополнительные файлы.\n",
    "\n",
    "- Решение каждой задачи/пункта задачи поместите после условия.\n",
    "\n",
    "- Не забывайте добавлять необходимые пояснения и комментарии.\n",
    "\n",
    "- В финальной версии, которая будет отправлена на проверку, должны быть удалены все отладочные артефакты. Под таким артефактами подразумеваются любые выводы ячеек, которые никак не прокоментированы в тексте, а также любой массовый/длинный технический вывод (даже если он прокомментирован в тексте).\n",
    "\n",
    "- При полном запуске решения (Kernel -> Restart & Run All) все ячейки должны выполняться без ошибок.\n",
    "\n",
    "- Максимальный балл за задание 100.\n",
    "\n",
    "- Часть задач помечена $\\triangle$. Они также входят в максимальный балл за задание, а значит являются обязательными для получения максимальной оценки, но мы считаем, что достаточно выполнить задания без $\\triangle$, чтобы вникнуть в основные вещи, происходящие в соотвествующей части задания.\n",
    "\n",
    "Желаем успехов!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a386bbbb",
   "metadata": {},
   "source": [
    "### Часть 1. Решаем задачу безусловной оптимизации\n",
    "\n",
    "Рассмотрим задачу минимизации эмпирического риска (да-да, machine learning):\n",
    "\\begin{equation}\n",
    "\\min_{w \\in \\mathbb{R}^d} \\frac{1}{n} \\sum\\limits_{i=1}^n \\ell (g(w, x_i), y_i) + \\frac{\\lambda}{2} \\| w \\|^2_2,\n",
    "\\end{equation}\n",
    "где $\\ell$ - функция потерь, $g$ - модель, $w$ - параметры модели, $\\{x_i, y_i\\}_{i=1}^n$ - выборка данных из векторов признаков $x_i$ и меток $y_i$, $\\lambda > 0$ - параметр регуляризации.\n",
    "\n",
    "Используем линейную модель $g(w, x) = w^T x$ и логистическую/сигмоидную функцию потерь: $\\ell(z,y) = \\ln (1 + \\exp(-yz))$ (__Важно: $y$ должен принимать значения $-1$ или $1$__). Полученная задача называется задачей логистической регрессии. \n",
    "\n",
    "__Задача 1. (всего 12 баллов)__ Проведем подготовительную работу. \n",
    "\n",
    "__а). (8 баллов)__ Выпишите градиент и гессиан для данной задачи. Является ли задача выпуклой? А $\\mu$ - сильно выпуклой? Если да, то как можно оценить $\\mu$? Оцените константу Липшица градиента $L$. "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "$\\newcommand{\\epsilon}{\\varepsilon}$\n",
    "$\\newcommand{\\phi}{\\varphi}$\n",
    "$\\newcommand{\\kappa}{\\varkappa}$\n",
    "$\\newcommand{\\le}{\\ \\leqslant\\ }$\n",
    "$\\newcommand{\\ge}{\\ \\geqslant\\ }$\n",
    "$\\newcommand{\\emptyset}{\\varnothing}$\n",
    "$\\newcommand{\\d}{\\partial}$\n",
    "$\\renewcommand{\\l}{\\left}$\n",
    "$\\renewcommand{\\r}{\\right}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5533fa57875d5ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для каждого из слагаемых суммы:\n",
    "$$f(w) = \\ell(g(w, x_i), y_i) = ln(1 + exp(- y_i\\langle w, x_i \\rangle)) = ln \\l( 1 + e^{-y_i\\langle w, x_i \\rangle} \\r)$$ \n",
    "Чтобы не тащить минус через все вычисления мерзкий минус и скалярное произведение, введём обозначение $$\\boxed{ y = -y_i,\\ \\ g = \\langle w, x_i \\rangle }$$\n",
    "$$\\d f(w) = \\dfrac{\\d e^{yg}}{1 + e^{yg}} = \\dfrac{\\d (yg) e^{yg}}{1 + e^{yg}} = \\dfrac{\\d(-y_i\\langle w, x_i \\rangle)e^{yg}}{1 + e^{yg}} = \\dfrac{y\\langle \\d w, x_i \\rangle e^{yg}}{1 + e^{yg}} = \\l\\langle \\dfrac{ye^{yg}\\cdot x_i}{1 + e^{yg}},\\ \\d w\\r\\rangle$$\n",
    "$$\\boxed{ \\nabla_w f(w) = \\dfrac{-y_ie^{-y_iw^Tx_i}}{1 + e^{-y_iw^Tx_i}}\\cdot x_i }$$\n",
    "\n",
    "$$\\d^2 f(w) = \\d \\l(\\l\\langle \\dfrac{ye^{yg}\\cdot x_i}{1 + e^{yg}},\\ \\d w_1\\r\\rangle\\r) = -y_i\\langle x_i, \\d w_1 \\rangle \\cdot \\d\\l( \\dfrac{e^{yg}}{1 + e^{yg}} \\r) = -y_i \\langle x_i, \\d w_1 \\rangle \\cdot \\l(\\dfrac{\\d e^{yg}}{(1 + e^{yg})^2} \\r) = -y_i \\langle x_i, \\d w_1 \\rangle \\cdot \\dfrac{y\\langle \\d w_2, x_i \\rangle e^{yg}}{(1 + e^{yg})^2} = $$\n",
    "$$ = -y_i \\langle x_i, \\d w_1 \\rangle \\cdot \\dfrac{-y_i\\langle x_i, \\d w_2 \\rangle e^{yg}}{(1 + e^{yg})^2} = \\l\\langle x_i \\cdot \\langle x_i, \\d w_1 \\rangle \\cdot \\dfrac{e^{yg}}{(1 + e^{yg})^2}, \\d w_2 \\r\\rangle = \\l\\langle ||x_i||_2^2 \\cdot \\dfrac{e^{yg}}{(1 + e^{yg})^2} \\d w_1, \\d w_2 \\r\\rangle$$\n",
    "$$\\boxed{ \\nabla_w^2 f(w) = ||x_i||_2^2 \\cdot \\dfrac{e^{-y_iw^Tx_i}}{(1 + e^{-y_iw^Tx_i})^2}}$$\n",
    "\n",
    "Значит, для \n",
    "$$F(w) = \\frac{1}{n} \\sum\\limits_{i=1}^n \\ell (g(w, x_i), y_i) + \\frac{\\lambda}{2} ||w||^2_2 = \\frac{1}{n} \\sum\\limits_{i=1}^n f(w) + \\frac{\\lambda}{2} ||w||^2_2$$ \n",
    "Имеем \n",
    "$$ \\boxed{ \\nabla_w F(w) = \\frac{1}{n} \\sum\\limits_{i=1}^n \\nabla_w f(w) + \\lambda w  = \\frac{1}{n} \\sum\\limits_{i=1}^n \\dfrac{-y_ie^{-y_iw^Tx_i}}{1 + e^{-y_iw^Tx_i}}\\cdot x_i  + \\lambda w } \\hspace{2cm} \\boxed{ \\nabla_w^2 F(w) = \\frac{1}{n} \\sum\\limits_{i=1}^n \\nabla_w^2 f(w) + \\lambda I = \\frac{1}{n} \\sum\\limits_{i=1}^n x_ix_i^T \\cdot \\dfrac{e^{-y_iw^Tx_i}}{(1 + e^{-y_iw^Tx_i})^2} + \\lambda I}$$\n",
    "\n",
    "$\\vspace{2cm}$\n",
    "\n",
    "Касательно $\\mu$-сильно выпуклости:\n",
    "$$\\nabla_w^2 F(w) - \\mu I = \\frac{1}{n} \\sum\\limits_{i=1}^n x_ix_i^T \\cdot \\dfrac{e^{-y_iw^Tx_i}}{(1 + e^{-y_iw^Tx_i})^2} + (\\lambda - \\mu)I$$\n",
    "Матрица $$A = \\frac{1}{n} \\sum\\limits_{i=1}^n x_ix_i^T \\cdot \\dfrac{e^{-y_iw^Tx_i}}{(1 + e^{-y_iw^Tx_i})^2}$$ положительно определена, ведь каждый множитель вида $\\dfrac{e^{-y_iw^Tx_i}}{(1 + e^{-y_iw^Tx_i})^2}$ положителен, и $a^Tx_ix_i^Ta = \\langle x_i^Ta,\\ x_i^Ta \\rangle > 0,\\ a \\ne 0$\n",
    "При $\\lambda < \\mu$ гарантировать $\\mu$-сильно выпуклость нельзя, но при $\\ \\lambda \\ge \\mu\\ $ мы её уже установили.\n",
    "\n",
    "Из тех же соображений, положительная определённость $\\nabla_w F(x)$ достигается при $\\lambda > 0$, что зашито в условии. Так что задача всегда выпуклая.\n",
    "\n",
    "$\\vspace{2cm}$\n",
    "\n",
    "$\\textbf{Липшицевость}$\n",
    "Для начала заметим банальность:\n",
    "$$0 < \\dfrac{e^{-y_iw^Tx_i}}{(1 + e^{-y_iw^Tx_i})^2} \\le \\dfrac{1}{4}$$\n",
    "В самом деле, $(1 + t)^2 - 4t = (1 - t)^2 \\ge 0 \\implies \\dfrac{t}{(1+t)^2} \\le \\dfrac{1}{4}$ \n",
    "Значит, $$||\\nabla_w F(w_1) - \\nabla_w F(w_2)||_2 \\le \\dfrac{1}{n} \\cdot \\sum_{i=1}^n\\dfrac{1}{4}||x_i||_2\\ +\\ \\lambda||w_1 - w_2||_2 $$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c759ddad4d3a769"
  },
  {
   "cell_type": "markdown",
   "id": "a61362fe",
   "metadata": {},
   "source": [
    "К заданию приложен датасет _mushrooms_. С помощью следующего кода сформируйте матрицу $X$ и вектор $y$, в которой и будет храниться выборка $\\{x_i, y_i\\}_{i=1}^n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "1d1f7763",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.402142100Z",
     "start_time": "2023-10-18T20:54:03.216861600Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = \"mushrooms.txt\" \n",
    "#файл должен лежать в той же директории, что и notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "41f4a066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.552141300Z",
     "start_time": "2023-10-18T20:54:03.225142100Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "data = load_svmlight_file(dataset)\n",
    "X, y = data[0].toarray(), data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ef9c8",
   "metadata": {},
   "source": [
    "Поменяем вектор $y$, чтобы $y_i$ принимали значения $-1$ и $1$. Вы также можете сделать дополнительную предобработку данных (приемами из машинного обучения), но это никак дополнительно не оценивается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "b9c6af0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.552141300Z",
     "start_time": "2023-10-18T20:54:03.321142600Z"
    }
   },
   "outputs": [],
   "source": [
    "y = 2 * y - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fabb63a",
   "metadata": {},
   "source": [
    "Разделим данные на две части: обучающую и тестовую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c8e9fd42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.552141300Z",
     "start_time": "2023-10-18T20:54:03.339143800Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d427e",
   "metadata": {},
   "source": [
    "__в). (4 балла)__ Для обучающей части $X_{train}$, $y_{train}$ оцените константу $L$. Задайте $\\lambda$ так, чтобы $\\lambda \\approx L / 1000$.  Реализуйте в коде подсчет значения, градиента и гессиана для нашей целевой функции ($X$, $y$, $\\lambda$ лучше подавать в качестве параметра, чтобы была возможность их менять, а не только подставлять фиксированные $X_{train}$, $y_{train}$). Можно использовать как библиотеку ``numpy``, так и библиотеки ``autograd``, ``pytorch``, ``jax``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "ec184b82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.554141700Z",
     "start_time": "2023-10-18T20:54:03.352144900Z"
    }
   },
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2377d566",
   "metadata": {},
   "source": [
    "__Задача 2. (всего 12 баллов)__ Данная часть задания связана с моментумом и ускорением.\n",
    "\n",
    "__а). (3 балла)__ Реализуйте метод тяжелого шарика и ускоренный градиентный метод Нестерова. \n",
    "\n",
    "На всякий случай мы приводим здесь вариант описания функции для градиентного спуска из первого задания. Можно пользоваться таким форматом по желанию. Учтите, что в коде встречается ``x_sol`` - это проблему стоит как-то обойти или не использовать критерии, завязанные на ``x_sol``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "6be5209a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.563144700Z",
     "start_time": "2023-10-18T20:54:03.370145800Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f, nabla_f, x_0, x_sol, gamma_k,\n",
    "                     K = 10**3, eps = 10**-5, mode = 'x_k - x^*'):\n",
    "    \"\"\"\n",
    "        f - целевая функция\n",
    "        nabla_f - градиент целевой функции\n",
    "        x_0 - стартовая точка\n",
    "        x_sol - точное решение (оно нужно для подсчета ошибки)\n",
    "        gamma_k - функция для вычисления шага метода\n",
    "        K - количество итераций (по умолчанию 1е3)\n",
    "        eps - желаемая точность (по умолчанию 1е-5)\n",
    "        mode - критерий сходимости \n",
    "               Значения либо 'x_k - x^*' - тогда критерий сходимости будет ||x_k - x^*||,\n",
    "               либо 'f(x_k) - f(x^*)' - тогда критерий сходимости будет f(x_k) - f(x^*),\n",
    "               либо 'x_k+1 - x_k', либо 'f(x_k+1) - f(x_k)' (критерии будут аналогичными)\n",
    "\n",
    "        Функция возвращает точку, в которой достигается минимум и вектор ошибок\n",
    "    \"\"\"\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "671dfd0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.564144100Z",
     "start_time": "2023-10-18T20:54:03.385147500Z"
    }
   },
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d708445",
   "metadata": {},
   "source": [
    "__б). (7 баллов)__ Решите задачу оптимизации на тестовой выборке с помощью двух реализованных методов. Зафиксируйте шаг $\\frac{1}{L}$ и перебирайте разные значения моментума от -1 до 1. Проверьте также значения моментума равные $\\frac{k}{k+3}$, $\\frac{k}{k+2}$, $\\frac{k}{k+1}$ ($k$ - номер итерации), а если целевая функция является  сильно выпуклой, то и $\\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}$. Стартовую точку и критерий сходимости можете выбрать на свой вкус, мы советуем использовать нормированную версию критерия, например, $\\frac{\\| \\nabla f(x^k) \\|}{\\| \\nabla f(x^0) \\|}$, а также использовать в Задачах 3-4 ту же самую стартовую точку и тот же самый критерий сходимости.\n",
    "\n",
    "В данном пункте нужно построить три графика: 1) значения критерия сходимости от номера итерации для метода тяжелого шарика с различными значениями моментума, 2) значения критерия сходимости от номера итерации для ускоренного градиентного метода с различными значениями моментума, 3) значения критерия сходимости от номера итерации для двух методов с наилучшим выбором моментума для каждого, а также градиентного спуска.\n",
    "\n",
    "Не забывайте делать выводы и комментировать результаты. Например, отразите всегда ли сходимость является монотонной?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "3ee49367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.565145100Z",
     "start_time": "2023-10-18T20:54:03.400142800Z"
    }
   },
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65373c6b",
   "metadata": {},
   "source": [
    "__в). $\\triangle$ (2 балла)__ В последние годы на практике (особенно в задачах машинного обучения) используется метод с \"моментумом\" в следующем виде:\n",
    "\\begin{align*}\n",
    "    g^{-1} &= \\nabla f(x^0)\n",
    "    \\\\\n",
    "    g^k &= \\alpha_k g^{k-1} + \\nabla f(x^k)\n",
    "    \\\\\n",
    "    x^{k+1} &= x^k - \\gamma_k g^k\n",
    "\\end{align*}\n",
    "Реализуйте метод с таким \"моментумом\". Попробуйте понять, как этот метод связан с методом тяжелого шарика и ускоренным градиентным методом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c95fa679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.565145100Z",
     "start_time": "2023-10-18T20:54:03.419144900Z"
    }
   },
   "outputs": [],
   "source": [
    "#ваше решение (Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19349804",
   "metadata": {},
   "source": [
    "__Задача 3. (всего 20 баллов)__ В этой части будем работать с методом сопряженных градиентов. \n",
    "\n",
    "__а). (5 балла)__ Реализуйте метод Флетчера-Ривса и Полака-Рибьера. Опишите, как будете искать шага $\\alpha_k$ (интересен как алгоритм, так и его инициализация). Добавьте в алгоритмы возможность делать \"рестарты\" (иногда брать $\\beta_k = 0$) с некоторой частотой, которую можно настраивать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "96dce8db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.565145100Z",
     "start_time": "2023-10-18T20:54:03.431143800Z"
    }
   },
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f0a52",
   "metadata": {},
   "source": [
    "__б). (6 баллов)__ Решите задачу оптимизации двумя реализованными методами, варьируя для каждого частоту \"рестартов\": $1$ (каждую итерацию $\\beta_k = 0$), $10$ (каждую десятую итерацию $\\beta_k = 0$), $100$, $1000$, без рестартов.\n",
    "\n",
    "Постройте три графика: 1) значения критерия сходимости от номера итерации для метода Флетчера-Ривса с различными частотами рестартов, 2) значения критерия сходимости от номера итерации для метода Полака-Рибьера с различными частотами рестартов, 3) значения критерия сходимости от номера итерации для обоих методов с наилучшим выбором частоты рестартов. Сделайте вывод."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d3970ae4e2a38f0f"
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "73a1a92f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.565145100Z",
     "start_time": "2023-10-18T20:54:03.448145300Z"
    }
   },
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efddfc99",
   "metadata": {},
   "source": [
    "__в). $\\triangle$ (9 баллов)__ В этом пункте отвлечемся от задачи регресии. Поисследуем особенности метода сопряженных градиентов для квадратичной задачи\n",
    "$$\\min_{x \\in \\mathbb{R}^d} \\left[\\tfrac{1}{2} x^T A x - b x \\right]$$\n",
    "с положительно определенной симметричной матрицей $A \\in \\mathbb{R}^{d \\times d}$ и некоторым вектором $b \\in \\mathbb{R}^d$. Нам нужно научиться генерировать матрицу $A$ с возможнностью задавать ее спектр (собственные значения). В прошлом задании уже просили сделать это. Мы советуем использовать следующий подход, основанный на разложении $A = Q D Q^T$, где матрица $D$ - диагональная, образованная из собственных значений, а $Q$ - ортогональная (ее можно сгенерировать с помощью $QR$-разложения случайной матрицы).\n",
    "\n",
    "Пусть у нас имеется квадратичная задача, у которой матрица $A \\in \\mathbb{R}^{d \\times d}$ имеет кластеризованные собственные значения, это означает, что существует некоторое число кластеров $k \\leq d$ и значения $\\tilde \\lambda_1 < \\ldots < \\tilde \\lambda_k$, что для любого $\\lambda_i$ собственного значения матрицы $A$ существует $j \\leq k$ такой, что $\\lambda_i \\in [(1 - p) \\tilde \\lambda_j; (1 + p) \\tilde \\lambda_j]$, где $p < 1$.\n",
    "\n",
    "Далее нужно будет генерировать кластеризованные собственные значения, а потом и матрицу $A$. Старайтесь при генерации спектра удостоверится, что все значения в нем разные. В качесве критерия сходимости используйте $\\frac{\\| x^k - x^* \\|^2_A} {\\| x^0 - x^* \\|^2_A}$, где $k$ - номер итерации, а $\\| x \\|^2_A = \\langle x, Ax \\rangle$. \n",
    "\n",
    "Протестируем работу метода сопряженных градиентов для различных вариантов кластеризации собственных значений:\n",
    "\n",
    "1) Пусть $d = 100$, $k = 2$, $p = 0,05$, $\\tilde \\lambda_1 = 1$, в кластерах для $\\tilde \\lambda_1$ и $\\tilde \\lambda_2$ находится по 50 собственных значений. Варьируйте значение $\\tilde \\lambda_2$ от $10$ до $10^5$ (5 различных значений достаточно). На одном графике отобразите значения критерия сходимости от номера итерации для каждого значения $\\tilde \\lambda_2$. Сделайте вывод.\n",
    "\n",
    "2) Пусть $d = 100$, $k = 2$, $p = 0,05$, $\\tilde \\lambda_1 = 1$, $\\tilde \\lambda_2 = 1000$. Варьируйте количество собственных значений в каждом из кластеров от $1$ до $99$ (5 различных значений достаточно). На одном графике отобразите значения критерия сходимости от номера итерации для каждого значения размера кластера для $\\tilde \\lambda_1$. Сделайте вывод.\n",
    "\n",
    "3) Пусть $d = 100$, $p = 0,05$, $\\tilde \\lambda_1 = 1$, $\\tilde \\lambda_k = 1000$. Варьируйте количество кластеров $k$ от 2 до 100 (5 различных значений достаточно, включите 100 - соотвествует равномерному распределению собственных значений). На одном графике отобразите значения критерия сходимости от номера итерации для каждого значения  $k$. Сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "44a6a09b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.565145100Z",
     "start_time": "2023-10-18T20:54:03.464144600Z"
    }
   },
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a4256",
   "metadata": {},
   "source": [
    "__Задача 4. (всего 17 баллов)__ Теперь поговорим про метод Ньютона и квазиньютоновские методы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc437ecf",
   "metadata": {},
   "source": [
    "__а). (4 балла)__ Для задачи регресии реализуйте классический метод Ньютона и запустите его. Сходится ли он? Если нет, то попробуйте перед использованием метода Ньютона сначала запускать метод градиентного спуска на несколько итераций. Варьируйте количество шагов градиентного спуска. Постройте график значения критерия сходимости от номера итерации для комбинации градиентного спуска и метода Ньютона с различным числом шагов градиентного спуска. Сделайте вывод. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "71e4edd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.565145100Z",
     "start_time": "2023-10-18T20:54:03.481143900Z"
    }
   },
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4941f7",
   "metadata": {},
   "source": [
    "__б). (4 балла)__ Для данной задачи реализуйте квазиньютоновский метод BFGS (можно реализовать более продвинутую версию L-BFGS, посмотрев оригинальную [статью](http://users.iems.northwestern.edu/~nocedal/PDFfiles/limited-memory.pdf)). Используйте его для решения задачи регресии. Добавьте его на график из предыдущего пункта. Сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "db1fbc2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.565145100Z",
     "start_time": "2023-10-18T20:54:03.496143700Z"
    }
   },
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4230c3",
   "metadata": {},
   "source": [
    "__в). $\\triangle$ (9 баллов)__ Снова отвелчемся от регресии и рассмотрим одномерную задачу минимизации:\n",
    "\\begin{equation}\n",
    "\\min_{x \\in \\mathbb{R}} f(x) = x \\arctan x - \\frac{1}{2} \\log (1 + x^2).\n",
    "\\end{equation}\n",
    "Решим эту задачу с помощью классического метода Ньютона. Нарисуйте графики сходимости метода для двух разных точек старта $x^0 = 1.3$ и $x^0 = 1.5$. Сделайте вывод.\n",
    "\n",
    "Чтобы добиться сходимости метода Ньютона необязательно прибегать к использованию другого метода в качестве стартового. Реализуйте две модификации метода Ньютона: демпфированный (добавление шага) и кубический метод Ньютона (смотрите [статью](https://link.springer.com/article/10.1007/s10107-006-0706-8)). Решают ли эти методы проблему сходимости метода Ньютона для стартовой точки $x^0 = 1.5$? В демпфированном методе попробуйте брать шаг от $0,5$ до $1$. Постройте графики сходимости. Сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e6ba1e2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.565145100Z",
     "start_time": "2023-10-18T20:54:03.513142900Z"
    }
   },
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7ed63",
   "metadata": {},
   "source": [
    "__Задача 5. $\\triangle$ (всего 5 баллов)__ Осталось объеденить результаты полученные в Задачах 1-4. Для этого вспомним, что исходная задача регрессии является задачой машинного обучения и с помощью линейной модели $g$ можно предсказывать значения меток $y$. Как использовать итоговую модель для предсказания? Ответив на вопрос, сделайте предсказания на тестовой выборке $X_{test}$. Сравните с реальными метками $y_{test}$. Количество правильно угаданных меток есть точность/accuracy модели. Сравните метод градиентного спуска, метод тяжелого шарика, ускоренный градиентный метод, метод Флетчера-Ривса, метод Полака-Рибьера, метод Ньютона, BFGS(L-BFGS). Постройте два графика: значение критерия сходимости от времени работы и точность предсказания от времени работы. Сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d3413c1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.566144800Z",
     "start_time": "2023-10-18T20:54:03.528143100Z"
    }
   },
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6e6dc",
   "metadata": {},
   "source": [
    "### Часть 2. Выпуклость\n",
    "\n",
    "__Задача 1. (4 балла)__ Пусть $ S \\subseteq \\mathbb{R}^d$ и пусть $\\|\\cdot\\|$ - норма на $\\mathbb{R}^d$.\n",
    "\n",
    "__а). (2 балла)__ Для $a \\geq 0$ определим множество $S_a$ как:\n",
    "$$ S_a = \\{x \\mid \\text{dist}(x, S) \\leq a \\},$$\n",
    "где \n",
    "$$\\text{dist}(x, S) = \\inf_{y \\in S} \\| x - y \\|.$$\n",
    "Множество $S_a$ называется расширенным на $a$ относительно $S$. Докажите, что если $S$ выпукло, то $S_a$ также выпукло."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Доказательство:\n",
    "Рассмотрим произвольные точки $x_1,\\ x_2 \\in S_a$ и их произвольную выпуклую комбинацию -- точку $\\ x = \\alpha\\cdot x_1 + (1 - \\alpha)\\cdot x_2\\ $ По определению $S_a$ :\n",
    "$$\\forall \\varepsilon > 0\\hspace{0.5cm} \\exists y_1,\\ y_2 \\in S \\hspace{0.5cm} ||x_1 - y_1|| < a + \\varepsilon,\\ ||x_2 - y_2|| < a + \\varepsilon$$\n",
    "Тогда для точки $y = \\alpha\\cdot y_1 + (1 - \\alpha)\\cdot y_2$, лежащей в $S$ в виду выпуклости, имеем:\n",
    "$$||x - y|| = ||\\alpha(x_1 - y_1) + (1-\\alpha)(x_2 - y_2)|| \\le \\alpha\\cdot||x_1 - y_1|| + (1-\\alpha)\\cdot||x_2 - y_2|| < a + \\varepsilon$$\n",
    "(Здесь использованы свойства нормы: неравенство треугольника и вынесение константы)\n",
    "Значит, в виду произвольности $\\varepsilon$ имеем $x \\in S_a$. В свою очередь в силу произвольности выбора $x$ на отрезке между $x_1$ и $x_2$ заключаем выпуклость $S_a. \\hspace{1cm} \\blacktriangleright$   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad9f448d2a429eb"
  },
  {
   "cell_type": "markdown",
   "id": "b97805a6",
   "metadata": {},
   "source": [
    "__б). (2 балла)__ Для $a \\geq 0$ определим множество $S_{-a}$ как:\n",
    "$$ S_{-a} = \\{x \\mid B(x, a) \\subset S\\}, $$\n",
    "где $B(x, a)$ - открытый шар (в норме $\\| \\cdot \\|$) с центром в $x$ и радиусом $a$. Множество $S_{-a}$ называется суженным на $a$ относительно $S$. Докажите, что если $S$ выпукло, то $S_{-a}$ также выпукло."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Доказательство:\n",
    "Рассмотрим произвольные точки $x_1,\\ x_2 \\in S_{-a}$ и их произвольную выпуклую комбинацию -- точку $x = \\alpha\\cdot x_1 + (1 - \\alpha)\\cdot x_2$. По определению $S_{-a}$ :\n",
    "$$B(x_1, a) \\subset S,\\ B(x_2, a) \\subset S$$\n",
    "Рассмотрим произвольную точку $y \\in B(x, a) \\implies ||x - y|| \\le a$. Покажем, что она лежит в $S$.\n",
    "Заметим, что точка $y_1 = x_1 + y - x \\in B(x_1, a)$, ведь $||x_1 - y_1|| = ||x - y|| \\le a$ и, аналогично, точка $y_2 = x_2 + y - x \\in B(x_2, a)$, ведь $||x_2 - y_2|| = ||x - y|| \\le a$.\n",
    "Значит, $y_1,\\ y_2 \\in S \\implies $ отрезок их соединяющий лежит целиком в $S$. \n",
    "Осталось заметить, что $y = \\alpha\\cdot y_1 + (1-\\alpha)\\cdot y_2 \\in S. \\hspace{1cm} \\blacktriangleright$ "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d7864d2eaea449b"
  },
  {
   "cell_type": "markdown",
   "id": "132395d6",
   "metadata": {},
   "source": [
    "__Задача 2. (3 балла)__ Пусть дана функция $f: \\mathbb{R}^d \\to \\mathbb{R}$. Выясните является ли функция выпуклой/$\\mu$-сильно выпуклой, если $f(x) = \\sum\\limits_{i=1}^{d} x_i^4$. В случае $\\mu$-сильной выпуклости нужно найти и $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Очевидно, $\\nabla^2 f(x)  = 12\\cdot diag(x_1^2, x_2^2, \\ldots, x_d^2)$ эта матрица положительно определена (из критерия Сильвестра), значит, функция выпукла. \n",
    "Отсюда же очевидно, что $\\mu$-сильно выпуклость гарантировать нельзя: $12\\cdot diag(x_1^2, x_2^2, \\ldots, x_d^2) - diag(1, 1, \\ldots, 1) \\nsucceq 0$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "671a1ec2e39774eb"
  },
  {
   "cell_type": "markdown",
   "id": "394d2fe9",
   "metadata": {},
   "source": [
    "__Задача 3. (всего 4 балла)__ Пусть дана функция $f: \\mathbb{S}^d \\to \\mathbb{R}$. Здесь $\\mathbb{S}$ - симметричные матрицы. Выясните является ли функция выпуклой/вогнутой, если\n",
    "\n",
    "__а). (2 балла)__ $f(X) = \\lambda_{\\max}(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Рассмотрим произвольные $X, Y \\in \\mathbb{S}^d$ и их произвольную выпуклую комбинацию -- матрицу $Z = \\alpha X + (1-\\alpha)Y \\implies Z \\in \\mathbb{S}^d$\n",
    "Пусть $\\lambda_{max}(X) = x,\\ \\lambda_{max}(Y) = y$. Имеем:\n",
    "\n",
    "$$\\forall v \\in \\mathbb{R}^d \\hspace{0.5cm} v^T(X - xI)v \\le 0$$\n",
    "$$\\forall u \\in \\mathbb{R}^d \\hspace{0.5cm} u^T(Y - yI)u \\le 0$$\n",
    "Значит, \n",
    "$$\\forall w \\in \\mathbb{R}^d \\hspace{0.5cm} 0 \\ge w^T\\cdot[\\alpha X - \\alpha xI + (1-\\alpha)Y - (1-\\alpha)yI]\\cdot w = w^T\\cdot[Z - (\\alpha\\cdot x + (1-\\alpha)\\cdot y)I]\\cdot w $$\n",
    "Откуда $\\lambda_{max}(Z) \\le \\alpha\\cdot x + (1-\\alpha)\\cdot y$\n",
    "В силу произвольности выбора матриц заключаем выпуклость"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "829d8a7372eace45"
  },
  {
   "cell_type": "markdown",
   "id": "89dd0197",
   "metadata": {},
   "source": [
    "__б). (2 балла)__ $f(X) = \\lambda_{\\min}(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Рассмотрим произвольные $X, Y \\in \\mathbb{S}^d$ и их произвольную выпуклую комбинацию -- матрицу $Z = \\alpha X + (1-\\alpha)Y \\implies Z \\in \\mathbb{S}^d$\n",
    "Пусть $\\lambda_{min}(X) = x,\\ \\lambda_{min}(Y) = y$. Имеем:\n",
    "\n",
    "$$\\forall v \\in \\mathbb{R}^d \\hspace{0.5cm} v^T(X - xI)v \\ge 0$$\n",
    "$$\\forall u \\in \\mathbb{R}^d \\hspace{0.5cm} u^T(Y - yI)u \\ge 0$$\n",
    "Значит, \n",
    "$$\\forall w \\in \\mathbb{R}^d \\hspace{0.5cm} 0 \\le w^T\\cdot[\\alpha X - \\alpha xI + (1-\\alpha)Y - (1-\\alpha)yI]\\cdot w = w^T\\cdot[Z - (\\alpha\\cdot x + (1-\\alpha)\\cdot y)I]\\cdot w $$\n",
    "Откуда $\\lambda_{min}(Z) \\ge \\alpha\\cdot x + (1-\\alpha)\\cdot y$\n",
    "В силу произвольности выбора матриц заключаем вогнутость"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1bf0483f51ae118"
  },
  {
   "cell_type": "markdown",
   "id": "a39d6ebf",
   "metadata": {},
   "source": [
    "__Задача 4. (3 балла)__ Выясните является ли функция $f: \\mathbb{S}^d_{++} \\to \\mathbb{R}$ выпуклой/вогнутой, если $f(X) = \\text{Tr}(X^{-1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для $\\ X \\in S_{++}^d$ :\n",
    "$$f(X) = Tr\\l(X^{-1}\\r) \\implies \\d f(X) = \\d Tr(X^{-1}) = Tr(\\d X^{-1}) = Tr(-X^{-1}(\\d X)X^{-1}) = -Tr(X^{-1}X^{-1}(\\d X))$$\n",
    "$$\\d f(X) = \\langle \\nabla f(x), \\d X \\rangle = Tr\\l(\\l(\\nabla f(X)^T\\r) \\cdot \\d X\\r) \\implies \\boxed{ \\nabla f(X) = -\\l(\\l( X^{-1}\\r)^2\\r)^T = -\\l( X^{-1}\\r)^2 =: -X^{-2} }$$\n",
    "\n",
    "$X, Y \\in S_{++}^d$\n",
    "\n",
    "$$f(Y) - f(X) -\\langle \\nabla f(X),\\ Y-X \\rangle  = Tr(Y^{-1}) - Tr(X^{-1}) + Tr\\l(X^{-2}\\l(Y - X\\r)\\r) = Tr(Y^{-1} - X^{-1} + X^{-2}Y + X^{-1}) = Tr(Y^{-1} + X^{-2}Y) \\ge 0$$\n",
    "\n",
    "Откуда заключаем выпуклость"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7560821be941d4a3"
  },
  {
   "cell_type": "markdown",
   "id": "5cbc6b7b",
   "metadata": {},
   "source": [
    "__Задача 5. (3 балла)__ Пусть дано множество $X \\subseteq \\mathbb{R}^d$ и $x^0 \\in X$. Докажите, что множество\n",
    "$$K(X, x^0)=\\left\\{ y \\in\\mathbb{R}^d \\mid y^T x^0 \\geq y^T x \\text{ for all } x \\in X\\right\\}$$ \n",
    "является выпуклым конусом."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сначала проверим, что $K := K(X, x_0)$ является конусом:\n",
    "Пусть $y \\in K,\\quad\\forall x\\in X,\\ \\forall\\lambda\\geqslant0: \\quad (\\lambda y)^Tx_0 = \\lambda\\cdot y^Tx_0 \\geqslant \\lambda\\cdot y^Tx \\implies\\lambda y\\in K$. $\\triangleright$\\\n",
    "Теперь проверим выпуклость $K$:\n",
    "Пусть $y_1, y_2 \\in K,\\quad\\forall x\\in X,\\ \\forall \\lambda\\in[0, 1]: (\\lambda y_1 + (1-\\lambda)y_2)^Tx_0 = \\lambda y_1^Tx_0 + (1-\\lambda)y_2^Tx_0 \\geqslant \\lambda y_1^Tx + (1-\\lambda)y_2^Tx \\implies (\\lambda y_1 + (1-\\lambda)y_2)\\in K \\hspace{1cm} \\blacktriangleright$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24b08e1e77a452ce"
  },
  {
   "cell_type": "markdown",
   "id": "d2f36ac4",
   "metadata": {},
   "source": [
    "__Задача 6. $\\triangle$ (6 баллов)__ Воспользовавшись неравенством Йенсена для выпуклой на $\\mathbb{R}_{++}$ функции $f(x) = -\\ln{x}$, докажите неравенство Гёльдера: $$\\sum\\limits_{i=1}^d x_i y_i \\le \\left( \\sum\\limits_{i=1}^d \\vert x_i\\vert ^p\\right)^{1/p} \\left( \\sum\\limits_{i=1}^d \\vert y_i\\vert^q\\right)^{1/q}$$ для $p >1, \\;\\; \\dfrac{1}{p} + \\dfrac{1}{q} = 1$. $\\mathbb{R}_{++}$ - положительные действительные числа."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Лемма (Неравенство Юнга).\n",
    "Для $a \\ge 0,\\ b \\ge 0,\\ p > 1,\\ q > 1 : \\frac{1}{p} + \\frac{1}{q} = 1\\ $ выполнено $\\ ab \\le \\frac{a^p}{p} + \\frac{b^q}{q}$.\n",
    "Доказательство:\n",
    "При $a = 0$ или $b = 0$ неравенство очевидно из неотрицательности обоих чисел. Отныне считаем, что $a > 0,\\ b > 0$.\n",
    "Функция $f(x) = -ln(x)$ является выпуклой на своей ОДЗ $\\mathbb{R}_{++}$. Тогда для $t = \\frac{1}{p} \\implies (1 - t) = \\frac{1}{q}$ имеем:\n",
    "$$ln\\left(\\frac{a^p}{p} + \\frac{b^q}{q}\\right) = ln(t\\cdot a^p + (1 - p)b^q) \\ge t\\cdot ln(a^p) + (1 - t)\\cdot ln(b^q) = t\\cdot p\\cdot ln(a) + (1 - t)\\cdot q\\cdot ln(b) = ln(a) + ln(b) = ln(ab)$$\n",
    "Откуда из монотонности логарифма следует доказываемое утверждение.\n",
    "Введём обозначение \n",
    "$$X = \\left(\\sum_{i=1}^d |x_i|^p\\right)^{\\frac{1}{p}},\\hspace{1cm} Y = \\left(\\sum_{i=1}^d |y_i|^p\\right)^{\\frac{1}{q}}$$\n",
    "Тогда из неравенства Юнга\n",
    "$$\\forall i \\in \\{1, \\ldots, d\\} :\\hspace{0.5cm} \\dfrac{|x_i|}{X}\\cdot \\dfrac{|y_i|}{Y} \\le \\dfrac{|x_i|^p}{p\\cdot X^p}\\cdot\\dfrac{|y_i|^q}{q\\cdot Y^q}$$\n",
    "Просуммировав по всем $i$ имеем\n",
    "$$\\sum_{i=1}^d \\dfrac{|x_i|\\cdot |y_i|}{X\\cdot Y} \\le \\frac{1}{p} + \\frac{1}{q} = 1\\hspace{0.5cm} \\implies \\hspace{0.5cm} \\sum_{i=1}^d |x_iy_i| \\le XY$$\n",
    "И тем более \\\n",
    "$$\\sum_{i=1}^d x_iy_i \\le \\left(\\sum_{i=1}^d |x_i|^p\\right)^{\\frac{1}{p}}\\cdot \\left(\\sum_{i=1}^d |y_i|^p\\right)^{\\frac{1}{q}} \\hspace{1cm} \\blacktriangleright$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d07efd8601a42da4"
  },
  {
   "cell_type": "markdown",
   "id": "3afed93c",
   "metadata": {},
   "source": [
    "__Задача 7. $\\triangle$ (6 баллов)__ Назовем множество $X \\subseteq \\mathbb{R}^d$ \"средневыпуклым\", если для любых его элементов $x$ и $y$ их середина также принадлежит $X$, т.е. $\\frac{x + y}{2} \\in X$. Докажите, что для замкнутых множеств \"средневыпуклость\" равносильна выпуклости."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Выпуклость очевидно влечёт в частности средневыпуклость.\n",
    "Докажем содержательную импликацию. Пусть $x,\\ y \\in X,\\ z = \\alpha\\cdot x + (1-\\alpha)\\cdot y$.\n",
    "По определению замкнутое множество содержит все свои предельные точки. Запустим следующий процесс:\n",
    "на каждом шаге мы делим текущий отрезок поплам (начиная с отрезка, соединящего $x$ и $y$). Из средневыпуклости - его середина лежит в $X$. Если середина совпадает с $z$, то завершаем процесс - получили требуемую принадлежность.\n",
    "Если же середина не совпадает с $z$, то берём в качестве рабочего отрезка ту половину текущего, которая содержит $z$. Вновь концы полученного отрезка - половинки лежат в $X$. Можем делать следующий шаг.\n",
    "Так, получим последовательность вложенных отрезков, которая по теореме Кантора имеет единственную общую точку $z$, но последовательность получаемых на шагах цикла середин отрезков имеет точку $z$ своим пределом, а, значит, из замкнутости $X$ имеем $z \\in X. \\hspace{1cm} \\blacktriangleright$ "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abe00ea155970af9"
  },
  {
   "cell_type": "markdown",
   "id": "4bd77bd8",
   "metadata": {},
   "source": [
    "__Задача 8. $\\triangle$ (6 баллов)__ Пусть $X = \\{x_1, \\ldots, x_{d+2}\\}$ - множество из $d + 2$ точек в $\\mathbb{R}^d$. Покажите, что $X$ можно разбить на два подмножества $S$ и $T = X \\setminus S$ таким образом, что пересечение их выпуклых оболочек (смотри определение в Части 7 пособия) непусто."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Это известная нам из курса дискретного анализа Теорема Радона.\n",
    "\n",
    "Рассмотрим веторы $v_2 = (x_2 - x_1),\\ v_3 = (x_3 - x_1),\\ ...,\\ v_{d+2} = (x_{d+2} - x_1)$.\\\n",
    " Их $d+1$ и все они в $\\mathbb{R}^d$, значит, они линейно зависимы, то есть \n",
    " $$\\exists\\ \\alpha_2, \\alpha_3, ..., \\alpha_{d+2} \\in \\mathbb{R}:\\quad \\sum_{i=2}^{d+2}\\alpha_i^2 > 0,\\quad \\sum_{i=2}^{d+2}\\alpha_i v_i = \\sum_{i=2}^{d+2}\\alpha_i (x_i - x_1) = 0$$\\\n",
    " Перепишем последнее равенство,  введя обозначение: $\\quad a_1 = -\\sum_{i=2}^{d+2}\\alpha_i,\\quad a_j = \\alpha_j$ при $j \\in \\{2, 3, ..., d+2\\}$\\\\\n",
    " $$\\sum_{i=1}^{d+2}a_i x_i = 0,\\quad \\text{ причём }\\quad \\sum_{i=1}^{d+2}a_i = a_1 + \\sum_{i=2}^{d+2}a_i = 0$$\\\n",
    " Разделим множество индексов $\\{1, 2, ..., d+2\\}$ на два непересекающихся подмножества $I$ и $J,\\ $ где $$\\quad\\forall i \\in I:\\ a_i > 0\\quad \\quad \\forall j \\in J:\\ a_j \\leqslant 0$$\\\n",
    " Пусть $$S = \\sum_{i \\in I}a_i = - \\sum_{j \\in J}a_j$$\\\n",
    " Если $S = 0$, то $\\forall k \\in \\{1, 2, ..., d+2\\}:\\ a_k = 0$, но тогда $\\sum_{i=2}^{d+2}\\alpha_i^2 = 0$, а мы изначально выбрали нетривиальную линейную комбинацию.\n",
    " Значит, $S > 0$. Рассмотрим точку $$x_0 = \\sum_{i \\in I}\\dfrac{a_i}{S}x_i = \\dfrac{1}{S}\\sum_{i=1}^{d+2}a_i x_i - \\sum_{j \\in J}\\dfrac{a_j}{S}x_j = \\sum_{j \\in J}\\dfrac{-a_j}{S}x_j$$\\\n",
    " Заметим, что точка $x_0$ есть выпуклая комбинация точек $\\{x_i\\}_{i \\in I}$, ведь $\\forall i \\in I:\\ 0 \\leqslant \\dfrac{a_i}{S} \\leqslant 1$ и $\\sum_{i \\in I}\\dfrac{a_i}{S} = \\dfrac{S}{S} = 1$\\\n",
    "  Значит, $x_0 \\in conv\\{x_i\\}_{i \\in I}$. Аналогично, $x_0 \\in conv\\{x_j\\}_{j \\in J}$. То есть точка $x_0$ лежит в пересечении обеих выпуклых оболочек. $\\hspace{1cm} \\blacktriangleright$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f584c2893b73e942"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
